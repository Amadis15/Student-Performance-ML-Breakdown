{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ec178a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6552d13b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('student_performance_dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a84bf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# types of columns we have\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f2f61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any dupes?\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eefea94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing values?\n",
    "df.notnull().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8249bad",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Create a Final_Score column\n",
    "sum_of_of_scores = df['math score'] + df['reading score'] + df['writing score']\n",
    "df['Final_Score'] = sum_of_of_scores / 3\n",
    "df['Final_Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f885fe",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# drop\n",
    "df = df.drop(columns=['math score', 'reading score', 'writing score'])\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e31adf",
   "metadata": {},
   "source": [
    "Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f72755",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.loc[:,df.columns != 'Final_Score']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfba85ac",
   "metadata": {},
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f588b87",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "labels = df['Final_Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb79ab1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# One hot encode categorical features\n",
    "features = pd.get_dummies(features)\n",
    "features.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a11715",
   "metadata": {},
   "source": [
    "train, test, split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740fd544",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3195de27",
   "metadata": {},
   "source": [
    "Create untrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b67c583",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeRegressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a426e74",
   "metadata": {},
   "source": [
    "Train model on the Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154610f5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "model.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d00fc7e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Compute Training Accuracy\n",
    "train_predictions = model.predict(features_train)\n",
    "print('Training Score :', mean_squared_error(labels_train, train_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d557bc",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Compute Test Accuracy\n",
    "test_predictions = model.predict(features_test)\n",
    "print('Testing Score:', mean_squared_error(labels_test, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d18d1e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "So my training score is 57.765\n",
    "While my testing score is 156.635\n",
    "This clearly demonstrates thatm model was not generalizing well to unseen data and its overfitting the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109e6c13",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "                        # MODEL 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988391c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Im going to regularize the tree and give it 5 stems of depth and min samples to split a node.\n",
    "model = DecisionTreeRegressor(max_depth= 5, min_samples_split= 10, min_samples_leaf= 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49306f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model on the training set \n",
    "model.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb408af9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Compute training accuracy \n",
    "train_predictions = model.predict(features_train)\n",
    "print('Training Score:', mean_squared_error(labels_train, train_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c245dab",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Compute test Accuracy\n",
    "test_predictions = model.predict(features_test)\n",
    "print('Testing ScoreL', mean_squared_error(labels_test, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776f7c26",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Training MSE is now 57.77\n",
    "Testing MSE is 156.63\n",
    "Nothing really changed by me regularizing the tree and giving it less stems. The gap between both error scores are still too large.\n",
    "I dont think that a single decision tree is going to give me my desired Generalization. Im going to go ahead and see if a Random Forest is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df492d9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "                    # Model 3(Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10759bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88087b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Model\n",
    "model = RandomForestRegressor(n_estimators= 100, max_depth= 5, random_state= 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d64f602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model on the training set\n",
    "model.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89445bfb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Compute training Accuracy \n",
    "train_predictions = model.predict(features_train)\n",
    "print('Training Score:', mean_squared_error(labels_train, train_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d884b558",
   "metadata": {},
   "source": [
    "Compute Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4631f855",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model.predict(features_test)\n",
    "print('Testing Score:', mean_squared_error(labels_test, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f0ce7d",
   "metadata": {},
   "source": [
    "Training MSE is now 75.988\n",
    "Testing MSE is 105.408\n",
    "For this Random Forest model, my generalization is way better. The test error has really improved over the original decision tree.\n",
    "The training error is a bit higher but since the testing error is lower now and similar to the training error, now I feel that the model is good.\n",
    "I say this because additionally, since my MSE is 105.408, then that means that my RMSE is 10.27. So my predictions are going to be about 10 points off\n",
    "out of 100. I feel like for this model where we are determining student scores, this should be good. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54483fe6",
   "metadata": {},
   "source": [
    "Now im going to deploy my model with joblip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4249fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(model, \"final_rf_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b13a997",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d30c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your trained model\n",
    "model = joblib.load(\"final_rf_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ec2fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.title(\"Student Final Score Predictor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b5f48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input form\n",
    "gender = st.selectbox(\"Gender\", [\"male\", \"female\"])\n",
    "lunch = st.selectbox(\"Lunch Type\", [\"standard\", \"free/reduced\"])\n",
    "test_prep = st.selectbox(\"Test Preparation Course\", [\"none\", \"completed\"])\n",
    "parent_edu = st.selectbox(\"Parental Education Level\", [\n",
    "    \"high school\", \"some college\", \"associate's degree\",\n",
    "    \"bachelor's degree\", \"master's degree\"\n",
    "])\n",
    "race_ethnicity = st.selectbox(\"Race/Ethnicity Group\", [\"group A\", \"group B\", \"group C\", \"group D\", \"group E\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5159913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare input as a DataFrame\n",
    "user_input = pd.DataFrame({\n",
    "    \"gender\": [gender],\n",
    "    \"lunch\": [lunch],\n",
    "    \"test preparation course\": [test_prep],\n",
    "    \"parental level of education\": [parent_edu],\n",
    "    \"race/ethnicity\": [race_ethnicity]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1bb5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode to match training\n",
    "user_input_encoded = pd.get_dummies(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccc191b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align with training features\n",
    "model_features = model.feature_names_in_  # only available in sklearn >= 1.0\n",
    "for col in model_features:\n",
    "    if col not in user_input_encoded.columns:\n",
    "        user_input_encoded[col] = 0\n",
    "user_input_encoded = user_input_encoded[model_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2bcf36",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Predict\n",
    "if st.button(\"Predict Final Score\"):\n",
    "    prediction = model.predict(user_input_encoded)[0]\n",
    "    st.success(f\"Estimated Final Score: **{round(prediction, 2)}** / 100\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
